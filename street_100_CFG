"""
Complete U-Net Flow Matching with Classifier-FREE Guidance Implementation
Enhanced with FID, Precision/Recall, and comprehensive evaluation
ADAPTED FOR SVHN DATASET with CFG
Modified: All outputs saved to sv_v3 folder structure
"""

import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torch.cuda.amp as amp

import torchvision
from torchvision import datasets, transforms
from torchvision.utils import make_grid, save_image
import torchvision.models as models

import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
import math
import os
import gc
import time
from typing import Tuple
from dataclasses import dataclass
from collections import namedtuple
from scipy import linalg
import warnings
warnings.filterwarnings('ignore')

try:
    from tqdm import tqdm
except ImportError:
    import subprocess
    import sys
    print("Installing tqdm...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "tqdm"])
    from tqdm import tqdm

# ============================================================================
# DEVICE SETUP
# ============================================================================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    print(f"GPU: {torch.cuda.get_device_name(0)}")

torch.manual_seed(42)
np.random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

# ============================================================================
# PRECISION-RECALL METRICS
# ============================================================================

Manifold = namedtuple("Manifold", ["features", "radii"])

class EfficientPrecisionRecall:
    """Memory and compute efficient Precision-Recall evaluator"""
    
    def __init__(self, k=3, device="cuda"):
        self.k = k
        self.device = device
        
        # Load VGG16
        self.vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)
        self.vgg16.classifier = self.vgg16.classifier[:4]
        self.vgg16.eval()
        
        for p in self.vgg16.parameters():
            p.requires_grad = False
        
        self.vgg16 = self.vgg16.to(device)
        
        if torch.cuda.is_available():
            self.vgg16 = self.vgg16.half()
    
    @torch.no_grad()
    def extract_features_fast(self, dataloader, max_samples=None):
        """Extract features with optimizations"""
        features_list = []
        samples_processed = 0
        
        for batch in tqdm(dataloader, desc="Extracting features"):
            if max_samples and samples_processed >= max_samples:
                break
            
            if isinstance(batch, (list, tuple)):
                batch = batch[0]
            
            if max_samples:
                remaining = max_samples - samples_processed
                batch = batch[:min(len(batch), remaining)]
            
            if batch.shape[-1] != 224:
                batch = F.interpolate(batch, size=(224, 224), 
                                    mode='bilinear', align_corners=False)
            
            batch = batch.to(self.device)
            
            if torch.cuda.is_available():
                batch = batch.half()
            
            features = self.vgg16(batch)
            features_list.append(features.float().cpu().numpy())
            
            samples_processed += len(batch)
        
        return np.concatenate(features_list, axis=0)
    
    def compute_distances_efficient(self, X, Y):
        """Compute distances using vectorized operations"""
        X = X.astype(np.float32)
        Y = Y.astype(np.float32)
        
        X = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-8)
        Y = Y / (np.linalg.norm(Y, axis=1, keepdims=True) + 1e-8)
        
        distances = 1 - np.dot(X, Y.T)
        
        return distances
    
    def compute_radii_fast(self, features):
        """Compute k-NN radii efficiently"""
        n = len(features)
        radii = np.zeros(n, dtype=np.float32)
        
        chunk_size = min(1000, n)
        
        for i in range(0, n, chunk_size):
            end_i = min(i + chunk_size, n)
            
            chunk_dists = self.compute_distances_efficient(
                features[i:end_i], features
            )
            
            for j in range(end_i - i):
                kth_dists = np.partition(chunk_dists[j], self.k + 1)[:self.k + 1]
                radii[i + j] = kth_dists[-1]
        
        return radii
    
    def compute_metrics_fast(self, ref_features, ref_radii, gen_features):
        """Compute precision and recall efficiently"""
        n_ref = len(ref_features)
        n_gen = len(gen_features)
        
        if n_ref * n_gen < 1e8:
            distances = self.compute_distances_efficient(ref_features, gen_features)
            precision = np.mean(np.any(distances < ref_radii[:, np.newaxis], axis=0))
            gen_radii = self.compute_radii_fast(gen_features)
            distances_T = distances.T
            recall = np.mean(np.any(distances_T < gen_radii[:, np.newaxis], axis=0))
        else:
            precision = self._compute_precision_batched(ref_features, ref_radii, gen_features)
            recall = self._compute_recall_batched(ref_features, gen_features)
        
        return precision, recall
    
    def _compute_precision_batched(self, ref_features, ref_radii, gen_features):
        n_gen = len(gen_features)
        batch_size = 100
        precision_count = 0
        
        for i in range(0, n_gen, batch_size):
            batch = gen_features[i:min(i + batch_size, n_gen)]
            dists = self.compute_distances_efficient(ref_features, batch)
            precision_count += np.sum(np.any(dists < ref_radii[:, np.newaxis], axis=0))
        
        return precision_count / n_gen
    
    def _compute_recall_batched(self, ref_features, gen_features):
        gen_radii = self.compute_radii_fast(gen_features)
        n_ref = len(ref_features)
        batch_size = 100
        recall_count = 0
        
        for i in range(0, n_ref, batch_size):
            batch = ref_features[i:min(i + batch_size, n_ref)]
            dists = self.compute_distances_efficient(gen_features, batch)
            recall_count += np.sum(np.any(dists < gen_radii[:, np.newaxis], axis=0))
        
        return recall_count / n_ref

# ============================================================================
# FID COMPUTATION - FIXED FOR MEMORY EFFICIENCY
# ============================================================================

class InceptionFeatureExtractor:
    """Extract features using pretrained InceptionV3 - MEMORY EFFICIENT"""
    
    def __init__(self, device='cuda'):
        self.device = device
        
        self.model = models.inception_v3(pretrained=True, transform_input=False)
        self.model.fc = nn.Identity()
        self.model.to(device)
        self.model.eval()
        
        self.normalize = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    
    @torch.no_grad()
    def extract_features(self, images, batch_size=16):
        """Extract InceptionV3 features from images - BATCH BY BATCH"""
        if images.dim() == 3:
            images = images.unsqueeze(0)
        
        features = []
        num_batches = (len(images) + batch_size - 1) // batch_size
        
        for i in tqdm(range(0, len(images), batch_size), 
                     desc="Extracting Inception features", 
                     total=num_batches):
            batch = images[i:i+batch_size]
            
            if batch.min() < 0:
                batch = (batch + 1) / 2
            
            if batch.shape[-1] != 299:
                batch = F.interpolate(batch, size=(299, 299), 
                                    mode='bilinear', align_corners=False)
            
            batch = batch.to(self.device)
            batch = self.normalize(batch)
            
            feat = self.model(batch)
            features.append(feat.cpu())
            
            del batch, feat
            if i % 3 == 0:
                torch.cuda.empty_cache()
        
        return torch.cat(features, dim=0)

def calculate_fid(real_features, fake_features):
    """Calculate FrÃ©chet Inception Distance"""
    mu1, sigma1 = real_features.mean(0), torch.cov(real_features.T)
    mu2, sigma2 = fake_features.mean(0), torch.cov(fake_features.T)
    
    mu1, sigma1 = mu1.numpy(), sigma1.numpy()
    mu2, sigma2 = mu2.numpy(), sigma2.numpy()
    
    diff = mu1 - mu2
    covmean = linalg.sqrtm(sigma1.dot(sigma2))
    
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    
    fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean)
    return fid

# ============================================================================
# FLOW MATCHING COMPONENTS
# ============================================================================

class CondOTFlowMatching:
    """Conditional Optimal Transport Flow Matching"""
    
    def __init__(self, sigma_min=0.001):
        self.sigma_min = sigma_min
    
    def sample_xt(self, x0, x1, t):
        """Linear interpolation between noise x0 and data x1"""
        t = t.view(-1, 1, 1, 1)
        return (1 - t) * x0 + t * x1
    
    def compute_ut(self, x0, x1):
        """Compute the conditional flow velocity: u_t = x1 - x0"""
        return x1 - x0

# ============================================================================
# U-NET COMPONENTS
# ============================================================================

def zero_module(module):
    """Zero out the parameters of a module."""
    for p in module.parameters():
        p.detach().zero_()
    return module

class GroupNorm32(nn.GroupNorm):
    def forward(self, x):
        return super().forward(x.float()).type(x.dtype)

def normalization(channels):
    """GroupNorm with 32 groups."""
    return GroupNorm32(32, channels)

def conv_nd(dims, *args, **kwargs):
    """Create a 1D, 2D, or 3D convolution layer."""
    if dims == 1:
        return nn.Conv1d(*args, **kwargs)
    elif dims == 2:
        return nn.Conv2d(*args, **kwargs)
    elif dims == 3:
        return nn.Conv3d(*args, **kwargs)
    raise ValueError(f"unsupported dimensions: {dims}")

def linear(*args, **kwargs):
    """Create a linear module."""
    return nn.Linear(*args, **kwargs)

class SiLU(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

def timestep_embedding(timesteps, dim, max_period=10000):
    """Create sinusoidal timestep embeddings."""
    half = dim // 2
    freqs = torch.exp(
        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half
    ).to(device=timesteps.device)
    args = timesteps[:, None].float() * freqs[None]
    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
    if dim % 2:
        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
    return embedding

class ResBlock(nn.Module):
    """Residual block with time embedding."""
    
    def __init__(
        self,
        channels,
        emb_channels,
        dropout,
        out_channels=None,
        use_scale_shift_norm=False,
        dims=2,
        up=False,
        down=False,
    ):
        super().__init__()
        self.channels = channels
        self.emb_channels = emb_channels
        self.dropout = dropout
        self.out_channels = out_channels or channels
        self.use_scale_shift_norm = use_scale_shift_norm

        self.in_layers = nn.Sequential(
            normalization(channels),
            SiLU(),
            conv_nd(dims, channels, self.out_channels, 3, padding=1),
        )

        self.updown = up or down

        if up:
            self.h_upd = nn.Upsample(scale_factor=2, mode="nearest")
            self.x_upd = nn.Upsample(scale_factor=2, mode="nearest")
        elif down:
            self.h_upd = nn.AvgPool2d(2)
            self.x_upd = nn.AvgPool2d(2)
        else:
            self.h_upd = self.x_upd = nn.Identity()

        self.emb_layers = nn.Sequential(
            SiLU(),
            linear(
                emb_channels,
                2 * self.out_channels if use_scale_shift_norm else self.out_channels,
            ),
        )
        
        self.out_layers = nn.Sequential(
            normalization(self.out_channels),
            SiLU(),
            nn.Dropout(p=dropout),
            zero_module(conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)),
        )

        if self.out_channels == channels:
            self.skip_connection = nn.Identity()
        else:
            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)

    def forward(self, x, emb):
        if self.updown:
            h = self.in_layers[:-1](x)
            h = self.h_upd(h)
            x = self.x_upd(x)
            h = self.in_layers[-1](h)
        else:
            h = self.in_layers(x)
            
        emb_out = self.emb_layers(emb).type(h.dtype)
        while len(emb_out.shape) < len(h.shape):
            emb_out = emb_out[..., None]
            
        if self.use_scale_shift_norm:
            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]
            scale, shift = torch.chunk(emb_out, 2, dim=1)
            h = out_norm(h) * (1 + scale) + shift
            h = out_rest(h)
        else:
            h = h + emb_out
            h = self.out_layers(h)
            
        return self.skip_connection(x) + h

class QKVAttention(nn.Module):
    """QKV attention mechanism."""
    
    def __init__(self, n_heads):
        super().__init__()
        self.n_heads = n_heads

    def forward(self, qkv):
        bs, width, length = qkv.shape
        assert width % (3 * self.n_heads) == 0
        ch = width // (3 * self.n_heads)
        q, k, v = qkv.chunk(3, dim=1)
        scale = 1 / math.sqrt(math.sqrt(ch))
        weight = torch.einsum(
            "bct,bcs->bts",
            (q * scale).view(bs * self.n_heads, ch, length),
            (k * scale).view(bs * self.n_heads, ch, length),
        )
        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)
        a = torch.einsum("bts,bcs->bct", weight, v.reshape(bs * self.n_heads, ch, length))
        return a.reshape(bs, -1, length)

class AttentionBlock(nn.Module):
    """Self-attention block."""
    
    def __init__(self, channels, num_heads=1, num_head_channels=-1):
        super().__init__()
        self.channels = channels
        if num_head_channels == -1:
            self.num_heads = num_heads
        else:
            assert channels % num_head_channels == 0
            self.num_heads = channels // num_head_channels
            
        self.norm = normalization(channels)
        self.qkv = conv_nd(1, channels, channels * 3, 1)
        self.attention = QKVAttention(self.num_heads)
        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))

    def forward(self, x):
        b, c, *spatial = x.shape
        x = x.reshape(b, c, -1)
        qkv = self.qkv(self.norm(x))
        h = self.attention(qkv)
        h = self.proj_out(h)
        return (x + h).reshape(b, c, *spatial)

# ============================================================================
# CONDITIONAL U-NET FOR CFG
# ============================================================================

class ConditionalUNetModel(nn.Module):
    """U-Net model with class conditioning for Classifier-Free Guidance."""
    
    def __init__(
        self,
        image_size=32,
        in_channels=3,
        model_channels=128,
        out_channels=3,
        num_res_blocks=2,
        attention_resolutions=(16, 8),
        dropout=0.0,
        channel_mult=(1, 2, 2, 2),
        dims=2,
        num_heads=4,
        num_head_channels=64,
        use_scale_shift_norm=True,
        resblock_updown=True,
        num_classes=11,  # 10 classes + 1 null class
    ):
        super().__init__()
        
        self.image_size = image_size
        self.in_channels = in_channels
        self.model_channels = model_channels
        self.out_channels = out_channels
        self.num_res_blocks = num_res_blocks
        self.attention_resolutions = attention_resolutions
        self.dropout = dropout
        self.channel_mult = channel_mult
        self.num_heads = num_heads
        self.num_head_channels = num_head_channels
        self.num_classes = num_classes

        # Time embedding
        time_embed_dim = model_channels * 4
        self.time_embed = nn.Sequential(
            linear(model_channels, time_embed_dim),
            SiLU(),
            linear(time_embed_dim, time_embed_dim),
        )
        
        # Class label embedding (11 classes: 0-9 + null class 10)
        self.label_embed = nn.Embedding(num_classes, time_embed_dim)

        ch = input_ch = int(channel_mult[0] * model_channels)
        self.input_blocks = nn.ModuleList([
            nn.Sequential(conv_nd(dims, in_channels, ch, 3, padding=1))
        ])
        input_block_chans = [ch]
        ds = 1
        
        for level, mult in enumerate(channel_mult):
            for _ in range(num_res_blocks):
                layers = [
                    ResBlock(
                        ch,
                        time_embed_dim,
                        dropout,
                        out_channels=int(mult * model_channels),
                        dims=dims,
                        use_scale_shift_norm=use_scale_shift_norm,
                    )
                ]
                ch = int(mult * model_channels)
                if ds in attention_resolutions:
                    layers.append(
                        AttentionBlock(
                            ch,
                            num_heads=num_heads,
                            num_head_channels=num_head_channels,
                        )
                    )
                self.input_blocks.append(nn.Sequential(*layers))
                input_block_chans.append(ch)
            if level != len(channel_mult) - 1:
                out_ch = ch
                self.input_blocks.append(
                    nn.Sequential(
                        ResBlock(
                            ch,
                            time_embed_dim,
                            dropout,
                            out_channels=out_ch,
                            dims=dims,
                            use_scale_shift_norm=use_scale_shift_norm,
                            down=True,
                        )
                        if resblock_updown
                        else nn.AvgPool2d(2)
                    )
                )
                ch = out_ch
                input_block_chans.append(ch)
                ds *= 2

        self.middle_block = nn.Sequential(
            ResBlock(
                ch,
                time_embed_dim,
                dropout,
                dims=dims,
                use_scale_shift_norm=use_scale_shift_norm,
            ),
            AttentionBlock(
                ch,
                num_heads=num_heads,
                num_head_channels=num_head_channels,
            ),
            ResBlock(
                ch,
                time_embed_dim,
                dropout,
                dims=dims,
                use_scale_shift_norm=use_scale_shift_norm,
            ),
        )

        self.output_blocks = nn.ModuleList([])
        for level, mult in list(enumerate(channel_mult))[::-1]:
            for i in range(num_res_blocks + 1):
                ich = input_block_chans.pop()
                layers = [
                    ResBlock(
                        ch + ich,
                        time_embed_dim,
                        dropout,
                        out_channels=int(model_channels * mult),
                        dims=dims,
                        use_scale_shift_norm=use_scale_shift_norm,
                    )
                ]
                ch = int(model_channels * mult)
                if ds in attention_resolutions:
                    layers.append(
                        AttentionBlock(
                            ch,
                            num_heads=num_heads,
                            num_head_channels=num_head_channels,
                        )
                    )
                if level and i == num_res_blocks:
                    out_ch = ch
                    layers.append(
                        ResBlock(
                            ch,
                            time_embed_dim,
                            dropout,
                            out_channels=out_ch,
                            dims=dims,
                            use_scale_shift_norm=use_scale_shift_norm,
                            up=True,
                        )
                        if resblock_updown
                        else nn.Upsample(scale_factor=2, mode="nearest")
                    )
                    ds //= 2
                self.output_blocks.append(nn.Sequential(*layers))

        self.out = nn.Sequential(
            normalization(ch),
            SiLU(),
            zero_module(conv_nd(dims, input_ch, out_channels, 3, padding=1)),
        )

    def forward(self, x, t, y):
        """
        Apply the model to an input batch with class conditioning.
        Args:
            x: (batch_size, channels, height, width)
            t: (batch_size,) timesteps
            y: (batch_size,) class labels (0-9 for SVHN, 10 for null)
        """
        # Combine time and label embeddings
        t_emb = self.time_embed(timestep_embedding(t, self.model_channels))
        label_emb = self.label_embed(y)
        emb = t_emb + label_emb  # Combine embeddings
        
        hs = []
        h = x
        for module in self.input_blocks:
            if isinstance(module, nn.Sequential):
                for layer in module:
                    if isinstance(layer, ResBlock):
                        h = layer(h, emb)
                    else:
                        h = layer(h)
            hs.append(h)

        for layer in self.middle_block:
            if isinstance(layer, ResBlock):
                h = layer(h, emb)
            else:
                h = layer(h)

        for module in self.output_blocks:
            h = torch.cat([h, hs.pop()], dim=1)
            for layer in module:
                if isinstance(layer, ResBlock):
                    h = layer(h, emb)
                else:
                    h = layer(h)

        return self.out(h)

# ============================================================================
# EMA
# ============================================================================

class EMA(nn.Module):
    def __init__(self, model, decay=0.9999):
        super().__init__()
        self.model = model
        self.decay = decay
        
        import copy
        self.ema_model = copy.deepcopy(model)
        for p in self.ema_model.parameters():
            p.requires_grad = False
        
        self.register_buffer('num_updates', torch.tensor(0))
    
    @torch.no_grad()
    def update_ema(self):
        """Update EMA parameters"""
        self.num_updates += 1
        decay = min(self.decay, (1 + self.num_updates) / (10 + self.num_updates))
        
        for ema_param, model_param in zip(self.ema_model.parameters(), self.model.parameters()):
            ema_param.data.mul_(decay).add_(model_param.data, alpha=1 - decay)
    
    def forward(self, *args, **kwargs):
        """Forward pass through the main model"""
        return self.model(*args, **kwargs)

# ============================================================================
# TRAINING FUNCTION WITH CFG
# ============================================================================

def train_flow_matching_cfg(
    model, 
    dataloader, 
    num_epochs=100,
    lr=2e-4,
    batch_size=64,
    physical_batch_size=16,
    use_ema=True,
    ema_decay=0.9999,
    grad_clip=1.0,
    cfg_dropout=0.1,  # Probability of dropping labels
    null_label=10,    # Null class for unconditional
    device='cuda'
):
    """Train Flow Matching model with Classifier-Free Guidance"""
    
    if use_ema:
        model = EMA(model, decay=ema_decay)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)
    scaler = amp.GradScaler()
    flow_matching = CondOTFlowMatching()
    
    accumulation_steps = batch_size // physical_batch_size
    
    model.train()
    losses = []
    
    for epoch in range(num_epochs):
        epoch_losses = []
        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')
        
        for batch_idx, (x1, labels) in enumerate(pbar):
            x1 = x1.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            
            optimizer.zero_grad(set_to_none=True)
            
            for i in range(0, x1.size(0), physical_batch_size):
                mini_x1 = x1[i:i+physical_batch_size]
                mini_labels = labels[i:i+physical_batch_size]
                mini_batch_size = mini_x1.size(0)
                
                # CFG: Randomly replace labels with null_label
                mask = torch.rand(mini_batch_size, device=device) < cfg_dropout
                mini_labels = torch.where(mask, 
                                         torch.full_like(mini_labels, null_label),
                                         mini_labels)
                
                x0 = torch.randn_like(mini_x1)
                t = torch.rand(mini_batch_size, device=device)
                
                xt = flow_matching.sample_xt(x0, mini_x1, t)
                ut = flow_matching.compute_ut(x0, mini_x1)
                
                with amp.autocast():
                    pred = model(xt, t, mini_labels)
                    loss = F.mse_loss(pred, ut) / accumulation_steps
                
                scaler.scale(loss).backward()
            
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
            scaler.step(optimizer)
            scaler.update()
            
            if use_ema:
                model.update_ema()
            
            epoch_losses.append(loss.item() * accumulation_steps)
            pbar.set_postfix({'loss': f'{loss.item() * accumulation_steps:.4f}'})
        
        scheduler.step()
        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        print(f'Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}')
        
        if (epoch + 1) % 5 == 0:
            os.makedirs('sv_v3/checkpoints', exist_ok=True)
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.ema_model.state_dict() if use_ema else model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }
            torch.save(checkpoint, f'sv_v3/checkpoints/flow_cfg_epoch_{epoch+1}.pt')
    
    return model, losses

# ============================================================================
# SAMPLING FUNCTIONS WITH CFG
# ============================================================================

@torch.no_grad()
def sample_flow_ode_cfg(model, num_samples, num_steps=50, batch_size=50, null_label=10, use_heun=True, device='cuda'):
    """Generate unconditional samples using CFG (null label)"""
    
    model.eval()
    if hasattr(model, 'ema_model'):
        sample_fn = model.ema_model
    else:
        sample_fn = model
    
    all_samples = []
    samples_generated = 0
    
    os.makedirs('sv_v3/generated_samples', exist_ok=True)
    
    pbar = tqdm(total=num_samples, desc="Generating unconditional samples (CFG)")
    
    while samples_generated < num_samples:
        curr_batch_size = min(batch_size, num_samples - samples_generated)
        
        x = torch.randn(curr_batch_size, 3, 32, 32, device=device)
        
        # Use null label for unconditional generation
        null_labels = torch.full((curr_batch_size,), null_label, device=device, dtype=torch.long)
        
        dt = 1.0 / num_steps
        
        for step in range(num_steps):
            t = step * dt
            t_next = min((step + 1) * dt, 1.0)
            t_tensor = torch.full((curr_batch_size,), t, device=device)
            
            if use_heun and step < num_steps - 1:
                with amp.autocast():
                    v_t = sample_fn(x, t_tensor, null_labels)
                x_euler = x + (t_next - t) * v_t
                
                t_next_tensor = torch.full((curr_batch_size,), t_next, device=device)
                with amp.autocast():
                    v_next = sample_fn(x_euler, t_next_tensor, null_labels)
                
                x = x + (t_next - t) * 0.5 * (v_t + v_next)
            else:
                with amp.autocast():
                    v_t = sample_fn(x, t_tensor, null_labels)
                x = x + (t_next - t) * v_t
        
        x_clamped = torch.clamp((x + 1) / 2, 0, 1)
        for i in range(curr_batch_size):
            sample_idx = samples_generated + i
            save_image(x_clamped[i], f'sv_v3/generated_samples/sample_{sample_idx:04d}.png')
        
        all_samples.append(x.cpu())
        samples_generated += curr_batch_size
        pbar.update(curr_batch_size)
        
        torch.cuda.empty_cache()
    
    pbar.close()
    return torch.cat(all_samples, dim=0)

@torch.no_grad()
def sample_with_cfg(
    flow_model,
    num_samples,
    class_labels,
    num_steps=50,
    guidance_scale=2.0,
    null_label=10,
    use_heun=True,
    device='cuda'
):
    """Sample using Classifier-Free Guidance for flow matching"""
    
    flow_model.eval()
    
    if hasattr(flow_model, 'ema_model'):
        sample_fn = flow_model.ema_model
    else:
        sample_fn = flow_model
    
    x = torch.randn(num_samples, 3, 32, 32, device=device)
    
    if isinstance(class_labels, int):
        class_labels = torch.full((num_samples,), class_labels, device=device, dtype=torch.long)
    elif isinstance(class_labels, list):
        class_labels = torch.tensor(class_labels, device=device, dtype=torch.long)
    
    dt = 1.0 / num_steps
    
    step_pbar = tqdm(range(num_steps), desc=f"CFG sampling (scale={guidance_scale})")
    
    for step in step_pbar:
        t = step * dt
        t_next = min((step + 1) * dt, 1.0)
        t_tensor = torch.full((num_samples,), t, device=device)
        
        # Get conditional and unconditional predictions
        with amp.autocast():
            # Conditional prediction
            v_cond = sample_fn(x, t_tensor, class_labels)
            
            # Unconditional prediction (use null label)
            null_labels = torch.full((num_samples,), null_label, device=device, dtype=torch.long)
            v_uncond = sample_fn(x, t_tensor, null_labels)
        
        # CFG formula: v_guided = (1 - w) * v_uncond + w * v_cond
        v_guided = (1 - guidance_scale) * v_uncond + guidance_scale * v_cond
        
        if use_heun and step < num_steps - 1:
            x_euler = x + (t_next - t) * v_guided
            
            t_next_tensor = torch.full((num_samples,), t_next, device=device)
            
            with amp.autocast():
                v_cond_next = sample_fn(x_euler, t_next_tensor, class_labels)
                v_uncond_next = sample_fn(x_euler, t_next_tensor, null_labels)
            
            v_next_guided = (1 - guidance_scale) * v_uncond_next + guidance_scale * v_cond_next
            
            x = x + (t_next - t) * 0.5 * (v_guided + v_next_guided)
        else:
            x = x + (t_next - t) * v_guided
        
        step_pbar.set_postfix({'t': f'{t:.3f}'})
    
    step_pbar.close()
    return x

# ============================================================================
# EVALUATION WITH FID AND PRECISION-RECALL
# ============================================================================

def evaluate_model_comprehensive_cfg(
    flow_model, 
    test_loader, 
    num_samples=50000,
    guidance_scales=[0.0, 1.0, 2.0, 4.0]
):
    """Comprehensive evaluation with FID and Precision-Recall using CFG"""
    
    print(f"\n{'='*70}")
    print(f"Comprehensive Model Evaluation with CFG")
    print(f"{'='*70}")
    
    print("Loading real samples for evaluation...")
    real_samples = []
    real_pbar = tqdm(test_loader, desc="Loading real samples")
    for images, _ in real_pbar:
        real_samples.append(images)
        current_count = len(torch.cat(real_samples))
        real_pbar.set_postfix({'samples': current_count, 'target': num_samples})
        if current_count >= num_samples:
            break
    real_samples = torch.cat(real_samples)[:num_samples]
    real_pbar.close()
    
    results = {}
    
    print("\n" + "="*70)
    print("Evaluating Unconditional Generation (CFG with null label)")
    print("="*70)
    
    print(f"Generating {num_samples} unconditional samples...")
    uncond_samples = sample_flow_ode_cfg(flow_model, num_samples, num_steps=50, batch_size=50, use_heun=True)
    
    print("Computing FID for unconditional samples...")
    feature_extractor = InceptionFeatureExtractor(device)
    
    real_features = feature_extractor.extract_features(real_samples, batch_size=16)
    torch.cuda.empty_cache()
    
    fake_features = feature_extractor.extract_features(uncond_samples, batch_size=16)
    torch.cuda.empty_cache()
    
    fid_uncond = calculate_fid(real_features, fake_features)
    
    print("Computing Precision-Recall for unconditional samples...")
    pr_eval = EfficientPrecisionRecall(k=3, device=device)
    real_loader = [(real_samples[i:i+64],) for i in range(0, len(real_samples), 64)]
    fake_loader = [(uncond_samples[i:i+64],) for i in range(0, len(uncond_samples), 64)]
    
    real_features_pr = pr_eval.extract_features_fast(real_loader, num_samples)
    real_radii = pr_eval.compute_radii_fast(real_features_pr)
    fake_features_pr = pr_eval.extract_features_fast(fake_loader, num_samples)
    
    precision_uncond, recall_uncond = pr_eval.compute_metrics_fast(
        real_features_pr, real_radii, fake_features_pr
    )
    
    results['unconditional'] = {
        'fid': fid_uncond,
        'precision': precision_uncond,
        'recall': recall_uncond
    }
    
    print(f"Unconditional Results:")
    print(f"   FID: {fid_uncond:.2f}")
    print(f"   Precision: {precision_uncond:.4f}")
    print(f"   Recall: {recall_uncond:.4f}")
    
    print("\n" + "="*70)
    print("Evaluating Classifier-FREE Guided Generation")
    print("="*70)
    
    eval_pbar = tqdm(guidance_scales, desc="Evaluating guidance scales")
    
    for guidance_scale in eval_pbar:
        eval_pbar.set_description(f"Evaluating guidance scale {guidance_scale}")
        
        print(f"\nGenerating {num_samples} samples with CFG scale {guidance_scale}...")
        guided_samples = []
        samples_per_class = num_samples // 10
        batch_size_per_class = 250
        
        for class_idx in range(10):
            class_samples = []
            for batch_start in range(0, samples_per_class, batch_size_per_class):
                batch_end = min(batch_start + batch_size_per_class, samples_per_class)
                batch_num_samples = batch_end - batch_start
                
                samples = sample_with_cfg(
                    flow_model,
                    num_samples=batch_num_samples,
                    class_labels=class_idx,
                    num_steps=50,
                    guidance_scale=guidance_scale,
                    null_label=10,
                    use_heun=True
                )
                class_samples.append(samples.cpu())
                torch.cuda.empty_cache()
            
            guided_samples.append(torch.cat(class_samples))
            torch.cuda.empty_cache()
        
        guided_samples = torch.cat(guided_samples)[:num_samples]
        
        print(f"Computing FID for guidance scale {guidance_scale}...")
        guided_features = feature_extractor.extract_features(guided_samples, batch_size=16)
        torch.cuda.empty_cache()
        
        fid_guided = calculate_fid(real_features, guided_features)
        
        print(f"Computing Precision-Recall for guidance scale {guidance_scale}...")
        guided_loader = [(guided_samples[i:i+64],) for i in range(0, len(guided_samples), 64)]
        guided_features_pr = pr_eval.extract_features_fast(guided_loader, num_samples)
        
        precision_guided, recall_guided = pr_eval.compute_metrics_fast(
            real_features_pr, real_radii, guided_features_pr
        )
        
        results[f'guided_{guidance_scale}'] = {
            'fid': fid_guided,
            'precision': precision_guided,
            'recall': recall_guided
        }
        
        eval_pbar.set_postfix({
            'FID': f'{fid_guided:.2f}',
            'P': f'{precision_guided:.3f}',
            'R': f'{recall_guided:.3f}'
        })
        
        print(f"Guidance Scale {guidance_scale} Results:")
        print(f"   FID: {fid_guided:.2f}")
        print(f"   Precision: {precision_guided:.4f}")
        print(f"   Recall: {recall_guided:.4f}")
        print("-" * 60)
        
        del guided_samples, guided_features, guided_features_pr
        torch.cuda.empty_cache()
    
    eval_pbar.close()
    
    return results

# ============================================================================
# VISUALIZATION
# ============================================================================

def visualize_samples_cfg(model, num_samples=64):
    """Visualize generated samples using CFG"""
    
    guidance_scales = [0.0, 1.0, 2.0, 4.0]
    
    os.makedirs('sv_v3/samples', exist_ok=True)
    
    print("\nGenerating visualization samples with CFG...")
    
    print("Generating unconditional samples...")
    uncond_samples = sample_flow_ode_cfg(model, num_samples, num_steps=50, batch_size=50, use_heun=True)
    grid = make_grid(uncond_samples, nrow=8, normalize=True, value_range=(-1, 1))
    save_image(grid, 'sv_v3/samples/unconditional_samples_cfg.png')
    
    guidance_pbar = tqdm(guidance_scales, desc="Generating CFG samples")
    for guidance_scale in guidance_pbar:
        guidance_pbar.set_description(f"CFG scale {guidance_scale}")
        
        class_labels = [i % 10 for i in range(num_samples)]
        
        samples = sample_with_cfg(
            model, num_samples, 
            class_labels=class_labels,
            num_steps=50, guidance_scale=guidance_scale, 
            null_label=10, use_heun=True
        )
        
        grid = make_grid(samples, nrow=8, normalize=True, value_range=(-1, 1))
        save_image(grid, f'sv_v3/samples/cfg_scale_{guidance_scale}.png')
    
    guidance_pbar.close()
    
    print("\nGenerating class-conditional samples...")
    class_pbar = tqdm(range(10), total=10)
    
    for class_idx in class_pbar:
        class_pbar.set_description(f"Generating digit {class_idx}")
        
        samples = sample_with_cfg(
            model, 16, class_labels=class_idx,
            num_steps=50, guidance_scale=2.0, 
            null_label=10, use_heun=True
        )
        
        grid = make_grid(samples, nrow=4, normalize=True, value_range=(-1, 1))
        save_image(grid, f'sv_v3/samples/digit_{class_idx}_cfg.png')
    
    class_pbar.close()
    print("All CFG samples saved to 'sv_v3/samples/' directory")

# ============================================================================
# MAIN FUNCTION
# ============================================================================

def main():
    print("="*80)
    print("SVHN Flow Matching with Classifier-FREE Guidance (CFG)")
    print("Complete Pipeline with FID and Precision-Recall Evaluation")
    print("="*80)
    
    # Configuration
    BATCH_SIZE = 64
    PHYSICAL_BATCH_SIZE = 16
    NUM_EPOCHS = 100
    NUM_SAMPLES = 50000
    CFG_DROPOUT = 0.1  # 10% chance to drop labels during training
    NULL_LABEL = 10     # Null class for unconditional generation
    
    # Create main sv_v3 directory structure
    os.makedirs('sv_v3', exist_ok=True)
    os.makedirs('sv_v3/checkpoints', exist_ok=True)
    os.makedirs('sv_v3/samples', exist_ok=True)
    os.makedirs('sv_v3/generated_samples', exist_ok=True)
    
    # Load SVHN Dataset
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    print("\nLoading SVHN dataset...")
    train_dataset = datasets.SVHN(
        root='./data', split='train', download=True, transform=transform
    )
    
    test_dataset = datasets.SVHN(
        root='./data', split='test', download=True, transform=transform
    )
    
    train_loader = DataLoader(
        train_dataset, batch_size=PHYSICAL_BATCH_SIZE, shuffle=True, 
        num_workers=2, pin_memory=True, persistent_workers=True
    )
    
    test_loader = DataLoader(
        test_dataset, batch_size=256, shuffle=False,
        num_workers=2, pin_memory=True
    )
    
    # Initialize CFG model
    print("\nInitializing Conditional U-Net for CFG...")
    flow_model = ConditionalUNetModel(
        image_size=32,
        in_channels=3,
        model_channels=128,
        out_channels=3,
        num_res_blocks=2,
        attention_resolutions=(16, 8),
        dropout=0.0,
        channel_mult=(1, 2, 2, 2),
        use_scale_shift_norm=True,
        resblock_updown=True,
        num_classes=11,  # 10 SVHN classes (0-9) + 1 null class (10)
    ).to(device)
    
    print(f"Flow model parameters: {sum(p.numel() for p in flow_model.parameters()):,}")
    
    # Train with CFG
    if os.path.exists('sv_v3/checkpoints/flow_cfg_final.pt'):
        print("Loading pre-trained CFG model...")
        checkpoint = torch.load('sv_v3/checkpoints/flow_cfg_final.pt', map_location=device)
        flow_model.load_state_dict(checkpoint['model_state_dict'])
    else:
        print(f"\nTraining CFG model for {NUM_EPOCHS} epochs...")
        print(f"CFG dropout probability: {CFG_DROPOUT}")
        print(f"Null label: {NULL_LABEL}")
        flow_model, _ = train_flow_matching_cfg(
            flow_model, train_loader, num_epochs=NUM_EPOCHS,
            batch_size=BATCH_SIZE, physical_batch_size=PHYSICAL_BATCH_SIZE,
            cfg_dropout=CFG_DROPOUT, null_label=NULL_LABEL,
        )
        os.makedirs('sv_v3/checkpoints', exist_ok=True)
        torch.save({
            'model_state_dict': flow_model.ema_model.state_dict() if hasattr(flow_model, 'ema_model') else flow_model.state_dict()
        }, 'sv_v3/checkpoints/flow_cfg_final.pt')
    
    print("\n" + "="*80)
    print("NO SEPARATE CLASSIFIER NEEDED WITH CFG!")
    print("="*80)
    
    # Comprehensive evaluation
    print("\n" + "="*80)
    print("COMPREHENSIVE EVALUATION WITH CFG")
    print("="*80)
    
    results = evaluate_model_comprehensive_cfg(
        flow_model, test_loader, 
        num_samples=NUM_SAMPLES,
        guidance_scales=[0.0, 1.0, 2.0, 4.0, 6.0]
    )
    
    # Visualize samples
    print("\n" + "="*80)
    print("GENERATING VISUALIZATIONS WITH CFG")
    print("="*80)
    
    visualize_samples_cfg(flow_model, num_samples=64)
    
    # Save comprehensive results
    print("\nSaving results...")
    with open('sv_v3/comprehensive_results_cfg.txt', 'w') as f:
        f.write("SVHN Classifier-FREE Guidance (CFG) - Comprehensive Results\n")
        f.write("="*70 + "\n\n")
        
        f.write("Method: Classifier-Free Guidance\n")
        f.write("- Single conditional U-Net model\n")
        f.write("- No separate classifier needed\n")
        f.write(f"- CFG dropout rate: {CFG_DROPOUT}\n")
        f.write(f"- Null label: {NULL_LABEL}\n\n")
        
        f.write("Unconditional Generation (using null label):\n")
        f.write(f"   FID: {results['unconditional']['fid']:.2f}\n")
        f.write(f"   Precision: {results['unconditional']['precision']:.4f}\n")
        f.write(f"   Recall: {results['unconditional']['recall']:.4f}\n\n")
        
        f.write("Classifier-Free Guided Generation:\n")
        for key in results:
            if key.startswith('guided_'):
                scale = key.split('_')[1]
                f.write(f"\nGuidance Scale {scale}:\n")
                f.write(f"   FID: {results[key]['fid']:.2f}\n")
                f.write(f"   Precision: {results[key]['precision']:.4f}\n")
                f.write(f"   Recall: {results[key]['recall']:.4f}\n")
    
    print("\n" + "="*80)
    print("COMPLETE!")
    print("="*80)
    print(f"Results saved to 'sv_v3/comprehensive_results_cfg.txt'")
    print(f"Visualizations saved to 'sv_v3/samples/' directory")
    print(f"Model checkpoints saved to 'sv_v3/checkpoints/' directory")
    print(f"Generated samples saved to 'sv_v3/generated_samples/' directory")
    print("\nKey advantages of Classifier-Free Guidance:")
    print("  - Single model (no separate classifier)")
    print("  - More memory efficient (no gradients needed)")
    print("  - Often better quality")
    print("  - Simpler training pipeline")
    
    return flow_model, results

if __name__ == "__main__":
    flow_model, results = main()
